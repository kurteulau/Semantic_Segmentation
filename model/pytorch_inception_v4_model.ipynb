{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    " ## Cloud Cover Semantic Segmentation\n",
    "### Final project: Inception v4 Model (as baseline)\n",
    "\n",
    "Project Team: <br>\n",
    "Kurt Eulau <br>\n",
    "Steve Hewitt <br>\n",
    "Tom Welsh <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 1: Import packages\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install pandas_path pytorch_lightning cloudpathlib loguru typer wandb albumentations# added wandb\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas_path in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.3.0)\n",
      "Requirement already satisfied: pytorch_lightning in /srv/conda/envs/notebook/lib/python3.8/site-packages (1.5.9)\n",
      "Requirement already satisfied: cloudpathlib in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.7.0)\n",
      "Requirement already satisfied: loguru in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.6.0)\n",
      "Requirement already satisfied: typer in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.4.0)\n",
      "Requirement already satisfied: wandb in /srv/conda/envs/notebook/lib/python3.8/site-packages (0.12.11)\n",
      "Requirement already satisfied: albumentations in /srv/conda/envs/notebook/lib/python3.8/site-packages (1.1.0)\n",
      "Requirement already satisfied: scipy in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (1.8.0)\n",
      "Requirement already satisfied: qudida>=0.0.4 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (0.0.4)\n",
      "Requirement already satisfied: PyYAML in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (5.4.1)\n",
      "Requirement already satisfied: numpy>=1.11.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (1.20.0)\n",
      "Requirement already satisfied: opencv-python-headless>=4.1.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (4.5.5.64)\n",
      "Requirement already satisfied: scikit-image>=0.16.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from albumentations) (0.19.1)\n",
      "Requirement already satisfied: scikit-learn>=0.19.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (1.0.2)\n",
      "Requirement already satisfied: typing-extensions in /srv/conda/envs/notebook/lib/python3.8/site-packages (from qudida>=0.0.4->albumentations) (4.0.1)\n",
      "Requirement already satisfied: imageio>=2.4.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.15.0)\n",
      "Requirement already satisfied: PyWavelets>=1.1.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (1.2.0)\n",
      "Requirement already satisfied: tifffile>=2019.7.26 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2022.2.9)\n",
      "Requirement already satisfied: packaging>=20.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (21.3)\n",
      "Requirement already satisfied: networkx>=2.2 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (2.6.3)\n",
      "Requirement already satisfied: pillow!=7.1.0,!=7.1.1,!=8.3.0,>=6.1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-image>=0.16.1->albumentations) (9.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from packaging>=20.0->scikit-image>=0.16.1->albumentations) (3.0.7)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (3.1.0)\n",
      "Requirement already satisfied: joblib>=0.11 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from scikit-learn>=0.19.1->qudida>=0.0.4->albumentations) (1.1.0)\n",
      "Requirement already satisfied: pandas>=0.23 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pandas_path) (1.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pandas>=0.23->pandas_path) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pandas>=0.23->pandas_path) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from python-dateutil>=2.8.1->pandas>=0.23->pandas_path) (1.16.0)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (4.62.3)\n",
      "Requirement already satisfied: future>=0.17.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (0.18.2)\n",
      "Requirement already satisfied: tensorboard>=2.2.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (2.8.0)\n",
      "Requirement already satisfied: pyDeprecate==0.3.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (0.3.1)\n",
      "Requirement already satisfied: setuptools==59.5.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (59.5.0)\n",
      "Requirement already satisfied: torch>=1.7.* in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (1.11.0)\n",
      "Requirement already satisfied: torchmetrics>=0.4.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (0.7.2)\n",
      "Requirement already satisfied: fsspec[http]!=2021.06.0,>=2021.05.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pytorch_lightning) (2022.1.0)\n",
      "Requirement already satisfied: aiohttp in /srv/conda/envs/notebook/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.8.1)\n",
      "Requirement already satisfied: requests in /srv/conda/envs/notebook/lib/python3.8/site-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.27.1)\n",
      "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.0)\n",
      "Requirement already satisfied: werkzeug>=0.11.15 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.0.3)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
      "Requirement already satisfied: absl-py>=0.4 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
      "Requirement already satisfied: wheel>=0.26 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
      "Requirement already satisfied: protobuf>=3.6.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.19.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (2.6.0)\n",
      "Requirement already satisfied: grpcio>=1.24.3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.43.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.7.2)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (5.0.0)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.0)\n",
      "Requirement already satisfied: zipp>=0.5 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.7.0)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.26.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.11)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
      "Requirement already satisfied: click<9.0.0,>=7.1.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from typer) (8.0.3)\n",
      "Requirement already satisfied: shortuuid>=0.5.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (1.0.8)\n",
      "Requirement already satisfied: yaspin>=1.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (2.1.0)\n",
      "Requirement already satisfied: pathtools in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (0.1.2)\n",
      "Requirement already satisfied: setproctitle in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (1.2.2)\n",
      "Requirement already satisfied: GitPython>=1.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (3.1.27)\n",
      "Requirement already satisfied: sentry-sdk>=1.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (1.5.8)\n",
      "Requirement already satisfied: docker-pycreds>=0.4.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (0.4.0)\n",
      "Requirement already satisfied: promise<3,>=2.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (2.3)\n",
      "Requirement already satisfied: psutil>=5.0.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from wandb) (5.9.0)\n",
      "Requirement already satisfied: gitdb<5,>=4.0.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from GitPython>=1.0.0->wandb) (4.0.9)\n",
      "Requirement already satisfied: smmap<6,>=3.0.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from gitdb<5,>=4.0.1->GitPython>=1.0.0->wandb) (5.0.0)\n",
      "Requirement already satisfied: termcolor<2.0.0,>=1.1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from yaspin>=1.0.0->wandb) (1.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.3.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.7.2)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (4.0.2)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /srv/conda/envs/notebook/lib/python3.8/site-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (6.0.2)\n"
     ]
    }
   ],
   "source": [
    "# Loading packages according to above may be problematic, reloading them below with an alternative method\n",
    "# https://jakevdp.github.io/blog/2017/12/05/installing-python-packages-from-jupyter/\n",
    "import sys\n",
    "!{sys.executable} -m pip install pandas_path pytorch_lightning cloudpathlib loguru typer wandb albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installing wandb can be difficult, you may need to 'pip install wandb' from the terminal \n",
    "!pip install wandb -qqq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pandas_path import path  # noqa\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import rasterio\n",
    "import pyproj\n",
    "import rioxarray\n",
    "import xrspatial.multispectral as ms\n",
    "from my_preprocessing_20220322 import *\n",
    "import segmentation_models_pytorch as smp\n",
    "from typing import Optional, List\n",
    "import warnings\n",
    "import albumentations as A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "<h2><font color='red'>Important: Change project parameter of Wandblogger below before every run<font></h2>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1a (Optional): Log training to wandb.ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_lightning.loggers import WandbLogger\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "wandb_logger = WandbLogger(project=\"inception_v4\", entity=\"w207-clouds\")  # Change this project paramater before every run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 2: Define working directories and global variables\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"/driven-data/cloud-cover\")\n",
    "TRAIN_FEATURES = DATA_DIR / \"train_features\"\n",
    "TRAIN_LABELS = DATA_DIR / \"train_labels\"\n",
    "\n",
    "assert TRAIN_FEATURES.exists()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BANDS = [\"B02\", \"B03\", \"B04\", \"B08\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>adwz</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwz</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11743</th>\n",
       "      <td>zxwv</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxwv</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11744</th>\n",
       "      <td>zxxo</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxxo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11745</th>\n",
       "      <td>zxym</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxym</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11746</th>\n",
       "      <td>zxza</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxza</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11747</th>\n",
       "      <td>zxzj</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxzj</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11748 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chip_id    location              datetime                   cloudpath\n",
       "0        adwp    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp\n",
       "1        adwu    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu\n",
       "2        adwz    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwz\n",
       "3        adxp    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp\n",
       "4        aeaj    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj\n",
       "...       ...         ...                   ...                         ...\n",
       "11743    zxwv  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxwv\n",
       "11744    zxxo  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxxo\n",
       "11745    zxym  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxym\n",
       "11746    zxza  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxza\n",
       "11747    zxzj  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxzj\n",
       "\n",
       "[11748 rows x 4 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_meta = pd.read_csv(DATA_DIR / \"train_metadata.csv\")\n",
    "display(train_meta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2a: Define Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create benchmark_src folder\n",
    "submission_dir = Path(\"benchmark_src\")\n",
    "if submission_dir.exists():\n",
    "    shutil.rmtree(submission_dir)\n",
    "\n",
    "submission_dir.mkdir(parents=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>Added way to apply Albumentations transforms in CloudDataset<font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_dataset.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_dataset.py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "import torch\n",
    "from typing import Optional, List\n",
    "import albumentations as A\n",
    "\n",
    "\n",
    "class CloudDataset(torch.utils.data.Dataset):\n",
    "    \"\"\"Reads in images, transforms pixel values, and serves a\n",
    "    dictionary containing chip ids, image tensors, and\n",
    "    label masks (where available).\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        x_paths: pd.DataFrame, # : syntax specifies datatype for each parameter for CloudDataset objects\n",
    "        bands: List[str],\n",
    "        y_paths: Optional[pd.DataFrame] = None,\n",
    "        transforms: Optional[list] = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudDataset class.\n",
    "\n",
    "        Args:\n",
    "            x_paths (pd.DataFrame): a dataframe with a row for each chip. There must be a column for chip_id,\n",
    "                and a column with the path to the TIF for each of bands\n",
    "            bands (list[str]): list of the bands included in the data\n",
    "            y_paths (pd.DataFrame, optional): a dataframe with a for each chip and columns for chip_id\n",
    "                and the path to the label TIF with ground truth cloud cover\n",
    "            transforms (list, optional): list of transforms to apply to the feature data (eg augmentations)\n",
    "        \"\"\"\n",
    "        self.data = x_paths\n",
    "        self.label = y_paths\n",
    "        self.transforms = transforms\n",
    "        self.bands = bands\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    # Similar to pop, helps iterate through dataset\n",
    "    def __getitem__(self, idx: int):\n",
    "        # Loads an n-channel image from a chip-level dataframe\n",
    "        img = self.data.loc[idx]\n",
    "        band_arrs = []\n",
    "        for band in self.bands:\n",
    "            with rasterio.open(img[f\"{band}_path\"]) as b:\n",
    "                band_arr = b.read(1).astype(\"float32\")\n",
    "            band_arrs.append(band_arr)\n",
    "        x_arr = np.stack(band_arrs, axis=-1) # 3-dimensional array\n",
    "\n",
    "        # Apply augmentations, if provided\n",
    "        if self.transforms:\n",
    "            # Subset each band's image to get 4 arrays that are each 512x512\n",
    "            band_02_arr = x_arr[:, :, 0]\n",
    "            band_03_arr = x_arr[:, :, 1]\n",
    "            band_04_arr = x_arr[:, :, 2]\n",
    "            band_08_arr = x_arr[:, :, 3]\n",
    "            \n",
    "            # Apply transform to each band's image\n",
    "            band_02_arr_transformed = self.transforms(image=band_02_arr)[\"image\"]\n",
    "            band_03_arr_transformed = self.transforms(image=band_03_arr)[\"image\"]\n",
    "            band_04_arr_transformed = self.transforms(image=band_04_arr)[\"image\"]\n",
    "            band_08_arr_transformed = self.transforms(image=band_08_arr)[\"image\"]\n",
    "            \n",
    "            # Recombine transformed images back into 512x512x4 ndarray\n",
    "            x_arr = np.dstack((band_02_arr_transformed,\n",
    "                               band_03_arr_transformed,\n",
    "                               band_04_arr_transformed,\n",
    "                               band_08_arr_transformed))\n",
    "            \n",
    "        # re-orders array to match expected format needed for model\n",
    "        x_arr = np.transpose(x_arr, [2, 0, 1]) \n",
    "\n",
    "        # Prepare dictionary for item\n",
    "        item = {\"chip_id\": img.chip_id, \"chip\": x_arr}\n",
    "        \n",
    "        # Spatial transforms are valid transforms to apply to label (unlike pixel transforms)\n",
    "        spatial_transforms = [A.augmentations.geometric.transforms.Affine,\n",
    "                    A.augmentations.crops.transforms.CenterCrop,\n",
    "                    A.augmentations.transforms.CoarseDropout,\n",
    "                    A.augmentations.crops.transforms.Crop,\n",
    "                    A.augmentations.crops.transforms.CropAndPad,\n",
    "                    A.augmentations.crops.transforms.CropNonEmptyMaskIfExists,\n",
    "                    A.augmentations.geometric.transforms.ElasticTransform,\n",
    "                    A.augmentations.transforms.Flip,\n",
    "                    A.augmentations.transforms.GridDistortion,\n",
    "                    A.augmentations.transforms.GridDropout,\n",
    "                    A.augmentations.transforms.HorizontalFlip,\n",
    "                    A.augmentations.transforms.Lambda,\n",
    "                    A.augmentations.geometric.resize.LongestMaxSize,\n",
    "                    A.augmentations.transforms.MaskDropout,\n",
    "                    A.augmentations.transforms.NoOp,\n",
    "                    A.augmentations.transforms.OpticalDistortion,\n",
    "                    A.augmentations.transforms.PadIfNeeded,\n",
    "                    A.augmentations.geometric.transforms.Perspective,\n",
    "                    A.augmentations.geometric.transforms.PiecewiseAffine,\n",
    "                    # A.augmentations.transforms.PixelDropout, # doesn't match docs for some reason\n",
    "                    A.augmentations.crops.transforms.RandomCrop,\n",
    "                    A.augmentations.crops.transforms.RandomCropNearBBox,\n",
    "                    A.augmentations.transforms.RandomGridShuffle,\n",
    "                    A.augmentations.crops.transforms.RandomResizedCrop,\n",
    "                    A.augmentations.geometric.rotate.RandomRotate90,\n",
    "                    A.augmentations.geometric.resize.RandomScale,\n",
    "                    A.augmentations.crops.transforms.RandomSizedBBoxSafeCrop,\n",
    "                    A.augmentations.crops.transforms.RandomSizedCrop,\n",
    "                    A.augmentations.geometric.resize.Resize,\n",
    "                    A.augmentations.geometric.rotate.Rotate,\n",
    "                    A.augmentations.geometric.rotate.SafeRotate,\n",
    "                    A.augmentations.geometric.transforms.ShiftScaleRotate,\n",
    "                    A.augmentations.geometric.resize.SmallestMaxSize,\n",
    "                    A.augmentations.transforms.Transpose,\n",
    "                    A.augmentations.transforms.VerticalFlip]\n",
    "        \n",
    "        # Load label if available\n",
    "        if self.label is not None:\n",
    "            label_path = self.label.loc[idx].label_path\n",
    "            with rasterio.open(label_path) as lp:\n",
    "                y_arr = lp.read(1).astype(\"float32\")\n",
    "            \n",
    "            # Apply data augmentations to the label - ONLY SPATIAL TRANSFORMS CAN BE APPLIED TO LABEL\n",
    "            if self.transforms:\n",
    "                \n",
    "                # Create list of valid spatial transforms from list of transforms applied to train images\n",
    "                valid_label_transforms = [transform for transform in self.transforms if type(transform) in spatial_transforms]\n",
    "                \n",
    "                # Apply only valid transforms to the label\n",
    "                self.transforms = A.Compose(valid_label_transforms)\n",
    "                y_arr = self.transforms(image=y_arr)[\"image\"]\n",
    "            \n",
    "            item[\"label\"] = y_arr\n",
    "\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><font color='red'>Added way to activate transforms in call to CloudDataset<font></h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/cloud_model.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/cloud_model.py\n",
    "from typing import Optional, List\n",
    "import albumentations as A\n",
    "import pandas as pd\n",
    "import pytorch_lightning as pl\n",
    "import segmentation_models_pytorch as smp\n",
    "import torch\n",
    "\n",
    "try:\n",
    "    from cloud_dataset import CloudDataset\n",
    "    from losses import intersection_over_union\n",
    "except ImportError:\n",
    "    from benchmark_src.cloud_dataset import CloudDataset\n",
    "    from benchmark_src.losses import intersection_over_union\n",
    "\n",
    "\n",
    "class CloudModel(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        bands: List[str],\n",
    "        x_train: Optional[pd.DataFrame] = None,\n",
    "        y_train: Optional[pd.DataFrame] = None,\n",
    "        x_val: Optional[pd.DataFrame] = None,\n",
    "        y_val: Optional[pd.DataFrame] = None,\n",
    "        hparams: dict = {},\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Instantiate the CloudModel class based on the pl.LightningModule\n",
    "        (https://pytorch-lightning.readthedocs.io/en/latest/common/lightning_module.html).\n",
    "\n",
    "        Args:\n",
    "            bands (list[str]): Names of the bands provided for each chip\n",
    "            x_train (pd.DataFrame, optional): a dataframe of the training features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_train (pd.DataFrame, optional): a dataframe of the training labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            x_val (pd.DataFrame, optional): a dataframe of the validation features with a row for each chip.\n",
    "                There must be a column for chip_id, and a column with the path to the TIF for each of bands.\n",
    "                Required for model training\n",
    "            y_val (pd.DataFrame, optional): a dataframe of the validation labels with a for each chip\n",
    "                and columns for chip_id and the path to the label TIF with ground truth cloud cover.\n",
    "                Required for model training\n",
    "            hparams (dict, optional): Dictionary of additional modeling parameters.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.hparams.update(hparams)\n",
    "        self.save_hyperparameters()\n",
    "\n",
    "        # required\n",
    "        self.bands = bands\n",
    "\n",
    "        # optional modeling params\n",
    "        self.backbone = self.hparams.get(\"backbone\", \"inceptionv4\")\n",
    "        self.weights = self.hparams.get(\"weights\", \"imagenet\")\n",
    "        self.learning_rate = self.hparams.get(\"lr\", 1e-3)\n",
    "        self.patience = self.hparams.get(\"patience\", 8)\n",
    "        self.num_workers = self.hparams.get(\"num_workers\", 2)\n",
    "        self.batch_size = self.hparams.get(\"batch_size\", 32)\n",
    "        self.gpu = self.hparams.get(\"gpu\", False)\n",
    "        \n",
    "        # Leave self.tranform = None in order to NOT apply augmentations in \n",
    "        # call to CloudDataset\n",
    "        self.transform = None\n",
    "        \n",
    "        # Uncomment line below and edit transforms in call to A.Compose to apply \n",
    "        # augmentations from Albumentations\n",
    "        # self.transform = A.Compose([A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.2, always_apply=False, p=0.5)])\n",
    "\n",
    "        # Instantiate datasets, model, and trainer params if provided\n",
    "        self.train_dataset = CloudDataset(\n",
    "            x_paths=x_train,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_train,\n",
    "            transforms=self.transform, # Will apply transforms if self.transform = A.Compose() is uncommented\n",
    "        )\n",
    "        self.val_dataset = CloudDataset(\n",
    "            x_paths=x_val,\n",
    "            bands=self.bands,\n",
    "            y_paths=y_val,\n",
    "            transforms=None,\n",
    "        )\n",
    "        self.model = self._prepare_model()\n",
    "\n",
    "    ## Required LightningModule methods ##\n",
    "\n",
    "    def forward(self, image: torch.Tensor):\n",
    "        # Forward pass\n",
    "        return self.model(image)\n",
    "\n",
    "    def training_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Training step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.train_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_train and y_train must be specified when CloudModel is instantiated to run training\"\n",
    "            )\n",
    "\n",
    "        # Switch on training mode\n",
    "        self.model.train()\n",
    "        torch.set_grad_enabled(True)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass\n",
    "        preds = self.forward(x)\n",
    "\n",
    "        # Log batch loss\n",
    "        loss = torch.nn.CrossEntropyLoss(reduction=\"none\")(preds, y).mean()\n",
    "        self.log(\n",
    "            \"loss\",\n",
    "            loss,\n",
    "            on_step=True,\n",
    "            on_epoch=True,\n",
    "            prog_bar=True,\n",
    "            logger=True,\n",
    "        )\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch: dict, batch_idx: int):\n",
    "        \"\"\"\n",
    "        Validation step.\n",
    "\n",
    "        Args:\n",
    "            batch (dict): dictionary of items from CloudDataset of the form\n",
    "                {'chip_id': list[str], 'chip': list[torch.Tensor], 'label': list[torch.Tensor]}\n",
    "            batch_idx (int): batch number\n",
    "        \"\"\"\n",
    "        if self.val_dataset.data is None:\n",
    "            raise ValueError(\n",
    "                \"x_val and y_val must be specified when CloudModel is instantiated to run validation\"\n",
    "            )\n",
    "\n",
    "        # Switch on validation mode\n",
    "        self.model.eval()\n",
    "        torch.set_grad_enabled(False)\n",
    "\n",
    "        # Load images and labels\n",
    "        x = batch[\"chip\"]\n",
    "        y = batch[\"label\"].long()\n",
    "        if self.gpu:\n",
    "            x, y = x.cuda(non_blocking=True), y.cuda(non_blocking=True)\n",
    "\n",
    "        # Forward pass & softmax\n",
    "        preds = self.forward(x)\n",
    "        preds = torch.softmax(preds, dim=1)[:, 1]\n",
    "        preds = (preds > 0.5) * 1  # convert to int\n",
    "\n",
    "        # Log batch IOU\n",
    "        batch_iou = intersection_over_union(preds, y)\n",
    "        self.log(\n",
    "            \"iou\", batch_iou, on_step=True, on_epoch=True, prog_bar=True, logger=True\n",
    "        )\n",
    "        return batch_iou\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        # DataLoader class for training\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=self.num_workers,\n",
    "            shuffle=True,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        # DataLoader class for validation\n",
    "        return torch.utils.data.DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            num_workers=0,\n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "        )\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = torch.optim.Adam(self.model.parameters(), lr=self.learning_rate)\n",
    "        sch = torch.optim.lr_scheduler.CosineAnnealingLR(opt, T_max=10)\n",
    "        return [opt], [sch]\n",
    "\n",
    "    ## Convenience Methods ##\n",
    "\n",
    "    def _prepare_model(self):\n",
    "        # Instantiate U-Net model\n",
    "        unet_model = smp.Unet(\n",
    "            encoder_name=self.backbone,\n",
    "            encoder_weights=self.weights,\n",
    "            in_channels=4,\n",
    "            classes=2,\n",
    "        )\n",
    "        if self.gpu:\n",
    "            unet_model.cuda()\n",
    "\n",
    "        return unet_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2b: Define Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_paths(df, feature_dir, label_dir=None, bands=BANDS):\n",
    "    \"\"\"\n",
    "    Given dataframe with a column for chip_id, returns a dataframe with a column\n",
    "    added indicating the path to each band's TIF image as \"{band}_path\", eg \"B02_path\".\n",
    "    A column is also added to the dataframe with paths to the label TIF, if the\n",
    "    path to the labels directory is provided.\n",
    "    \"\"\"\n",
    "    for band in bands:\n",
    "        df[f\"{band}_path\"] = feature_dir / df[\"chip_id\"] / f\"{band}.tif\"\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[f\"{band}_path\"].path.exists().all()\n",
    "    if label_dir is not None:\n",
    "        df[\"label_path\"] = label_dir / (df[\"chip_id\"] + \".tif\")\n",
    "        # make sure a random sample of paths exist\n",
    "        assert df.sample(n=40, random_state=5)[\"label_path\"].path.exists().all()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing benchmark_src/losses.py\n"
     ]
    }
   ],
   "source": [
    "%%file {submission_dir}/losses.py\n",
    "import numpy as np\n",
    "\n",
    "# Loss function\n",
    "def intersection_over_union(pred, true):\n",
    "    \"\"\"\n",
    "    Calculates intersection and union for a batch of images.\n",
    "\n",
    "    Args:\n",
    "        pred (torch.Tensor): a tensor of predictions\n",
    "        true (torc.Tensor): a tensor of labels\n",
    "\n",
    "    Returns:\n",
    "        intersection (int): total intersection of pixels\n",
    "        union (int): total union of pixels\n",
    "    \"\"\"\n",
    "    valid_pixel_mask = true.ne(255)  # valid pixel mask\n",
    "    true = true.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "    pred = pred.masked_select(valid_pixel_mask).to(\"cpu\")\n",
    "\n",
    "    # Intersection and union totals\n",
    "    intersection = np.logical_and(true, pred)\n",
    "    union = np.logical_or(true, pred)\n",
    "    return intersection.sum() / union.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the cloud model we just saved.\n",
    "from benchmark_src.cloud_model import CloudModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 3: Preprocessing\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3a: Establish a standard collection of chips for model testing--remove bad chips before starting\n",
    "\n",
    "Upon visual inspection, we discovered that some chips whose labels indicated \"no cloud\" actually had clearly defineable clouds in them, and some chips whose labels indicated \"100% cloud\" were actually not completely covered by clouds.\n",
    "\n",
    "While it was not possible to inspect every pixel of every chip in the dataset, we felt that the most egregious labelling errors from the total universe of chips under consideration should be removed before they were split into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our initial universe consists of 11748 chips.\n"
     ]
    }
   ],
   "source": [
    "train_meta = add_paths(train_meta, TRAIN_FEATURES, TRAIN_LABELS)\n",
    "train_meta.shape  # this is the original size of our universe of chips\n",
    "print(f\"Our initial universe consists of {train_meta.shape[0]} chips.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create our own copy of the data called \"full_set\" for subsequent use\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "full_set = train_meta.copy()\n",
    "full_set_x=full_set[feature_cols].copy()\n",
    "full_set_y=full_set[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # This function allows you to display a chip by name (useful for debugging)\n",
    "\n",
    "# display_true_color_label_pixel_count('byka', full_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# display_true_color_label_pixel_count('qslz', full_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "##### Collecting lists of mislabelled chips and put them in csv files..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Our focus was on removing chips that indicated that incorrectly labelled \"no cloud\" or \"fully cloud\":\n",
    "\n",
    "# -Some chips were labelled as having 0 cloud pixels when the clearly had clouds upon visual inspection\n",
    "\n",
    "# -Some chips were labelled as every pixel was a cloud, even though the chip did not have full cloud cover\n",
    "\n",
    "# # This cell takes about 10-12 minutes to run--leave this cell and the ones below commented out unless you want to\n",
    "# # recount the list of chips whose cells are either 0 clouds or all clouds. Since these outputs are written to csv files\n",
    "# # below, they do not need to be rerun each time, the information can be pulled directly from the csv files.\n",
    "\n",
    "# list_of_no_cloud_chips = []\n",
    "# list_of_all_cloud_chips = []\n",
    "# for index, row in full_set_y.iterrows():\n",
    "#     index_id = index\n",
    "#     chip_id = row[0]\n",
    "#     label_path = row[1]\n",
    "#     with Image.open(label_path) as im:\n",
    "#         label_arr = np.array(im)\n",
    "#         ones_count = np.count_nonzero(label_arr)\n",
    "#         if ones_count == 0:\n",
    "#             list_of_no_cloud_chips.append(chip_id)\n",
    "#         if ones_count == 512*512:\n",
    "#             list_of_all_cloud_chips.append(chip_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Prints a list of chips labelled \"NO CLOUD\"\n",
    "# print('list of chips labelled \"NO CLOUD:\"')\n",
    "# print(list_of_no_cloud_chips)\n",
    "\n",
    "# # Prints a list of chips labelled \"ALL CLOUD\"\n",
    "# print('\\nlist of chips labelled \"ALL CLOUD:\"')\n",
    "# print(list_of_all_cloud_chips)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # Writing lists of 'no cloud' and 'all cloud' chips to csv files (leave this section commented out after data cleaned)...\n",
    "\n",
    "# # Store chips with no clouds in 'no_cloud_chips.csv'\n",
    "\n",
    "# list_of_no_cloud_chips_df = pd.DataFrame(list_of_no_cloud_chips, columns=['chip_id'])\n",
    "# list_of_no_cloud_chips_df.to_csv('no_cloud_chips.csv', index=False, header=False)\n",
    "# with open('no_cloud_chips.csv', 'w+', newline='') as file:     \n",
    "#     write = csv.writer(file, delimiter='\\n', lineterminator='\\n') \n",
    "#     write.writerow(list_of_no_cloud_chips) \n",
    "\n",
    "\n",
    "# # Store chips with no clouds in 'no_cloud_chips.csv'\n",
    "\n",
    "# list_of_all_cloud_chips_df = pd.DataFrame(list_of_all_cloud_chips, columns=['chip_id'])\n",
    "# list_of_all_cloud_chips_df.to_csv('all_cloud_chips.csv', index=False, header=False)\n",
    "# with open('all_cloud_chips.csv', 'w+', newline='') as file:     \n",
    "#     write = csv.writer(file, delimiter='\\n', lineterminator='\\n') \n",
    "#     write.writerow(list_of_all_cloud_chips) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visually inspect chips whose labels have 0 cloud pixels and manually record mislabelled chips in a separate csv file (leave this section commented out after data cleaned)\n",
    "\n",
    "# # Select chips from csv files for display\n",
    "# with open('no_cloud_chips.csv', newline='') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     no_cloud_list = list(reader)\n",
    "\n",
    "# # flatten list\n",
    "# flat_no_cloud_list = [item for sublist in no_cloud_list for item in sublist]\n",
    "# print(f\"There are {len(flat_no_cloud_list)} chips out of the original {full_set.shape[0]} whose labels indicate 0 cloud pixels.\") \n",
    "# # print(flat_no_cloud_list)   # Uncomment this line to see a listing of chips whose labels have 0 cloud pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # subset a dataframe of just those chips\n",
    "# inspect_no_clouds_set = full_set.loc[full_set['chip_id'].isin(flat_no_cloud_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # visually inspect these chips in groups of 100, and record (in a spreadsheet) which images clearly have clouds--\n",
    "# # these are mislabelled and may throw off our models (it may take a minute or two to display)\n",
    "\n",
    "# inspect_no_clouds_subset = inspect_no_clouds_set[0:100]  # adjust this slice index to display [100:200], [200:300], etc.\n",
    "\n",
    "# fig, axs = plt.subplots(20, 5, figsize=(20,80))\n",
    "# fig.tight_layout()\n",
    "# counter = 0\n",
    "# for i in range(20):\n",
    "#     for j in range(5):\n",
    "#         example_chip = inspect_no_clouds_subset.iloc[counter]\n",
    "#         im = true_color_img(example_chip.chip_id)\n",
    "#         axs[i,j].imshow(im)\n",
    "#         axs[i,j].tick_params(left=False, labelleft=False, right=False)\n",
    "#         axs[i,j].patch.set_visible(False)\n",
    "#         axs[i,j].set_title(example_chip.chip_id, fontsize=20)\n",
    "#         axs[i,j].axis('off')\n",
    "#         counter += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visually inspect chips whose labels have 100% (512 x 512 = 262,144) cloud pixels and manually record mislabelled chips in a separate csv file \n",
    "\n",
    "# # Select chips from csv files for display\n",
    "# with open('all_cloud_chips.csv', newline='') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     all_cloud_list = list(reader)\n",
    "\n",
    "# # flatten list\n",
    "# flat_all_cloud_list = [item for sublist in all_cloud_list for item in sublist]\n",
    "# print(f\"There are {len(flat_all_cloud_list)} chips out of the original {full_set.shape[0]} whose labels indicate every pixel is a cloud.\") \n",
    "# # print(flat_no_cloud_list)   # Uncomment this line to see a listing of chips whose labels have 0 cloud pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # subset a dataframe of just those chips\n",
    "# inspect_all_clouds_set = full_set.loc[full_set['chip_id'].isin(flat_all_cloud_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # visually inspect these chips in groups of 100, and record (in a spreadsheet) which images clearly have clouds--\n",
    "# # these are mislabelled and may throw off our models\n",
    "\n",
    "# inspect_all_clouds_subset = inspect_all_clouds_set[0:100]  # adjust this slice index to see [100:200], [200:300], etc.\n",
    "\n",
    "# fig, axs = plt.subplots(20, 5, figsize=(20,80))\n",
    "# fig.tight_layout()\n",
    "# counter = 0\n",
    "# for i in range(20):\n",
    "#     for j in range(5):\n",
    "#         example_chip = inspect_all_clouds_subset.iloc[counter]\n",
    "#         im = true_color_img(example_chip.chip_id)\n",
    "#         axs[i,j].imshow(im)\n",
    "#         axs[i,j].tick_params(left=False, labelleft=False, right=False)\n",
    "#         axs[i,j].patch.set_visible(False)\n",
    "#         axs[i,j].set_title(example_chip.chip_id, fontsize=20)\n",
    "#         axs[i,j].axis('off')\n",
    "#         counter += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Visually inspect the chips to be rejected\n",
    "# Note: indexes in the rejection_list_set[0:205] were rejected because they were labelled \"no clouds\" when they had clouds.  Higher indexes were rejected because they were labelled \"100% clouds\" when clearly not every pixel was a cloud.\n",
    "\n",
    "# # Select chips from csv files for display\n",
    "# with open('incorrectly_labeled_chips.csv', newline='') as f:\n",
    "#     reader = csv.reader(f)\n",
    "#     all_cloud_list = list(reader)\n",
    "\n",
    "# # flatten list\n",
    "# rejection_list = [item for sublist in all_cloud_list for item in sublist]\n",
    "# print(f\"There are {len(rejection_list)} chips out of the original {full_set.shape[0]} whose labels indicate every pixel is a cloud.\") \n",
    "# # print(flat_no_cloud_list)   # Uncomment this line to see a listing of chips whose labels have 0 cloud pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # subset a dataframe of just those chips\n",
    "# rejection_list_set = full_set.loc[full_set['chip_id'].isin(rejection_list)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # visually inspect these chips in groups of 100, and record (in a spreadsheet) which images clearly have clouds--\n",
    "# # these are mislabelled and may throw off our models\n",
    "\n",
    "# rejection_list_subset = rejection_list_set[0:100]  # adjust this slice index to see [100:200], [200:300], etc.\n",
    "\n",
    "# fig, axs = plt.subplots(20, 5, figsize=(20,80))\n",
    "# fig.tight_layout()\n",
    "# counter = 0\n",
    "# for i in range(20):\n",
    "#     for j in range(5):\n",
    "#         example_chip = rejection_list_subset.iloc[counter]\n",
    "#         im = true_color_img(example_chip.chip_id)\n",
    "#         axs[i,j].imshow(im)\n",
    "#         axs[i,j].tick_params(left=False, labelleft=False, right=False)\n",
    "#         axs[i,j].patch.set_visible(False)\n",
    "#         axs[i,j].set_title(example_chip.chip_id, fontsize=20)\n",
    "#         axs[i,j].axis('off')\n",
    "#         counter += 1\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3b:  Remove the mislabeled chips from the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chip_id</th>\n",
       "      <th>location</th>\n",
       "      <th>datetime</th>\n",
       "      <th>cloudpath</th>\n",
       "      <th>B02_path</th>\n",
       "      <th>B03_path</th>\n",
       "      <th>B04_path</th>\n",
       "      <th>B08_path</th>\n",
       "      <th>label_path</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>adwp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>adwu</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adwu</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adwu/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adwu.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>adxp</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/adxp</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/adxp/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/adxp.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>aeaj</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeaj</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeaj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeaj.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>aeap</td>\n",
       "      <td>Chifunfu</td>\n",
       "      <td>2020-04-29T08:20:47Z</td>\n",
       "      <td>az://./train_features/aeap</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeap/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeap/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeap/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/aeap/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/aeap.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11741</th>\n",
       "      <td>zxuw</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxuw</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxuw/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxuw/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxuw/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxuw/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/zxuw.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11742</th>\n",
       "      <td>zxvi</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxvi</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxvi/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxvi/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxvi/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxvi/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/zxvi.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11744</th>\n",
       "      <td>zxxo</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxxo</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxxo/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxxo/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxxo/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxxo/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/zxxo.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11745</th>\n",
       "      <td>zxym</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxym</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxym/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxym/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxym/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxym/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/zxym.tif</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11747</th>\n",
       "      <td>zxzj</td>\n",
       "      <td>Launceston</td>\n",
       "      <td>2020-09-06T00:08:20Z</td>\n",
       "      <td>az://./train_features/zxzj</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxzj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxzj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxzj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_features/zxzj/B...</td>\n",
       "      <td>/driven-data/cloud-cover/train_labels/zxzj.tif</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10986 rows Ã— 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      chip_id    location              datetime                   cloudpath  \\\n",
       "0        adwp    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwp   \n",
       "1        adwu    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adwu   \n",
       "3        adxp    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/adxp   \n",
       "4        aeaj    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeaj   \n",
       "5        aeap    Chifunfu  2020-04-29T08:20:47Z  az://./train_features/aeap   \n",
       "...       ...         ...                   ...                         ...   \n",
       "11741    zxuw  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxuw   \n",
       "11742    zxvi  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxvi   \n",
       "11744    zxxo  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxxo   \n",
       "11745    zxym  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxym   \n",
       "11747    zxzj  Launceston  2020-09-06T00:08:20Z  az://./train_features/zxzj   \n",
       "\n",
       "                                                B02_path  \\\n",
       "0      /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1      /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "3      /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4      /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "5      /driven-data/cloud-cover/train_features/aeap/B...   \n",
       "...                                                  ...   \n",
       "11741  /driven-data/cloud-cover/train_features/zxuw/B...   \n",
       "11742  /driven-data/cloud-cover/train_features/zxvi/B...   \n",
       "11744  /driven-data/cloud-cover/train_features/zxxo/B...   \n",
       "11745  /driven-data/cloud-cover/train_features/zxym/B...   \n",
       "11747  /driven-data/cloud-cover/train_features/zxzj/B...   \n",
       "\n",
       "                                                B03_path  \\\n",
       "0      /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1      /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "3      /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4      /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "5      /driven-data/cloud-cover/train_features/aeap/B...   \n",
       "...                                                  ...   \n",
       "11741  /driven-data/cloud-cover/train_features/zxuw/B...   \n",
       "11742  /driven-data/cloud-cover/train_features/zxvi/B...   \n",
       "11744  /driven-data/cloud-cover/train_features/zxxo/B...   \n",
       "11745  /driven-data/cloud-cover/train_features/zxym/B...   \n",
       "11747  /driven-data/cloud-cover/train_features/zxzj/B...   \n",
       "\n",
       "                                                B04_path  \\\n",
       "0      /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1      /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "3      /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4      /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "5      /driven-data/cloud-cover/train_features/aeap/B...   \n",
       "...                                                  ...   \n",
       "11741  /driven-data/cloud-cover/train_features/zxuw/B...   \n",
       "11742  /driven-data/cloud-cover/train_features/zxvi/B...   \n",
       "11744  /driven-data/cloud-cover/train_features/zxxo/B...   \n",
       "11745  /driven-data/cloud-cover/train_features/zxym/B...   \n",
       "11747  /driven-data/cloud-cover/train_features/zxzj/B...   \n",
       "\n",
       "                                                B08_path  \\\n",
       "0      /driven-data/cloud-cover/train_features/adwp/B...   \n",
       "1      /driven-data/cloud-cover/train_features/adwu/B...   \n",
       "3      /driven-data/cloud-cover/train_features/adxp/B...   \n",
       "4      /driven-data/cloud-cover/train_features/aeaj/B...   \n",
       "5      /driven-data/cloud-cover/train_features/aeap/B...   \n",
       "...                                                  ...   \n",
       "11741  /driven-data/cloud-cover/train_features/zxuw/B...   \n",
       "11742  /driven-data/cloud-cover/train_features/zxvi/B...   \n",
       "11744  /driven-data/cloud-cover/train_features/zxxo/B...   \n",
       "11745  /driven-data/cloud-cover/train_features/zxym/B...   \n",
       "11747  /driven-data/cloud-cover/train_features/zxzj/B...   \n",
       "\n",
       "                                           label_path  \n",
       "0      /driven-data/cloud-cover/train_labels/adwp.tif  \n",
       "1      /driven-data/cloud-cover/train_labels/adwu.tif  \n",
       "3      /driven-data/cloud-cover/train_labels/adxp.tif  \n",
       "4      /driven-data/cloud-cover/train_labels/aeaj.tif  \n",
       "5      /driven-data/cloud-cover/train_labels/aeap.tif  \n",
       "...                                               ...  \n",
       "11741  /driven-data/cloud-cover/train_labels/zxuw.tif  \n",
       "11742  /driven-data/cloud-cover/train_labels/zxvi.tif  \n",
       "11744  /driven-data/cloud-cover/train_labels/zxxo.tif  \n",
       "11745  /driven-data/cloud-cover/train_labels/zxym.tif  \n",
       "11747  /driven-data/cloud-cover/train_labels/zxzj.tif  \n",
       "\n",
       "[10986 rows x 9 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A total of 762 chips that were deemed mislabled have been removed the original 11748 chips.  Our full_set dataset now contains 10986 chips.\n"
     ]
    }
   ],
   "source": [
    "full_set_cleaned = remove_chips(full_set, 'incorrectly_labeled_chips.csv') # train_meta renamed to this smaller group\n",
    "display(full_set_cleaned)\n",
    "print(f\"A total of {full_set.shape[0]-full_set_cleaned.shape[0]} chips that were deemed mislabled have been removed the original {full_set.shape[0]} chips.  Our full_set dataset now contains {full_set_cleaned.shape[0]} chips.\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='split-data'></a>\n",
    "\n",
    "#### Step 3c: Split the data  \n",
    "\n",
    "To train our model, we want to separate the data into a \"training\", \"validation\", and \"test\" set. That way we'll have a portion of labelled data that was not used in model training, which can give us a more accurate sense of how our model will perform on the competition test data. Remember, none of the test set locations are in competition training data, so your model's will performance will ultimately be measured on unseen locations.\n",
    "\n",
    "For each geography, we split our chips randomly into 60% training, 20% validation and 20% test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, val, test = train_val_test_split(full_set_cleaned, column='location', pct_train=0.6, pct_val=0.2, pct_test=0.2, random_state=42)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10986, 9), (6556, 9), (2198, 9), (2232, 9))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_set_cleaned.shape, train.shape, val.shape, test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate features from labels\n",
    "feature_cols = [\"chip_id\"] + [f\"{band}_path\" for band in BANDS]\n",
    "\n",
    "train_x = train[feature_cols].copy()\n",
    "train_y = train[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "val_x = val[feature_cols].copy()\n",
    "val_y = val[[\"chip_id\", \"label_path\"]].copy()\n",
    "\n",
    "test_x = test[feature_cols].copy()\n",
    "test_y = test[[\"chip_id\", \"label_path\"]].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Step 4: Model Training\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"http://data.lip6.fr/cadene/pretrainedmodels/inceptionv4-8e4777a0.pth\" to /home/jovyan/.cache/torch/hub/checkpoints/inceptionv4-8e4777a0.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a524ff0ebe4a6592e31636933a2641",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/163M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True, used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n"
     ]
    }
   ],
   "source": [
    "## Set up pytorch_lightning.Trainer object\n",
    "cloud_model = CloudModel(\n",
    "    bands=BANDS,\n",
    "    x_train=train_x,\n",
    "    y_train=train_y,\n",
    "    x_val=val_x,\n",
    "    y_val=val_y,\n",
    "    hparams={\"num_workers\": 7, \"batch_size\": 16},\n",
    ")\n",
    "checkpoint_callback = pl.callbacks.ModelCheckpoint(\n",
    "    monitor=\"iou_epoch\", mode=\"max\", verbose=True\n",
    ")\n",
    "early_stopping_callback = pl.callbacks.early_stopping.EarlyStopping(\n",
    "    monitor=\"iou_epoch\",\n",
    "    patience=(cloud_model.patience),\n",
    "    mode=\"max\",\n",
    "    verbose=True,\n",
    ")\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    gpus=1,\n",
    "    fast_dev_run=False,\n",
    "    callbacks=[checkpoint_callback, early_stopping_callback],\n",
    "    logger = wandb_logger\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mw207-clouds\u001b[0m (use `wandb login --relogin` to force relogin)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/jovyan/ucb_mids_w207_final_project/notebooks/wandb/run-20220329_044201-1r4fxc54</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/w207-clouds/inception_v4/runs/1r4fxc54\" target=\"_blank\">jumping-firefly-1</a></strong> to <a href=\"https://wandb.ai/w207-clouds/inception_v4\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 2917564 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 1029436 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 978254 bytes\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Serializing object of type DataFrame that is 345230 bytes\n",
      "\n",
      "  | Name  | Type | Params\n",
      "-------------------------------\n",
      "0 | model | Unet | 48.8 M\n",
      "-------------------------------\n",
      "48.8 M    Trainable params\n",
      "0         Non-trainable params\n",
      "48.8 M    Total params\n",
      "195.169   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation sanity check: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ea2ca302963464f815c387e4cc03600",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved. New best score: 0.869\n",
      "Epoch 0, global step 409: iou_epoch reached 0.86902 (best 0.86902), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=0-step=409.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.020 >= min_delta = 0.0. New best score: 0.889\n",
      "Epoch 1, global step 819: iou_epoch reached 0.88887 (best 0.88887), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=1-step=819.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2, global step 1229: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.004 >= min_delta = 0.0. New best score: 0.893\n",
      "Epoch 3, global step 1639: iou_epoch reached 0.89276 (best 0.89276), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=3-step=1639.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4, global step 2049: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.003 >= min_delta = 0.0. New best score: 0.896\n",
      "Epoch 5, global step 2459: iou_epoch reached 0.89575 (best 0.89575), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=5-step=2459.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.007 >= min_delta = 0.0. New best score: 0.903\n",
      "Epoch 6, global step 2869: iou_epoch reached 0.90319 (best 0.90319), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=6-step=2869.ckpt\" as top 1\n",
      "wandb: Network error (ReadTimeout), entering retry loop.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7, global step 3279: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.003 >= min_delta = 0.0. New best score: 0.906\n",
      "Epoch 8, global step 3689: iou_epoch reached 0.90578 (best 0.90578), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=8-step=3689.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Metric iou_epoch improved by 0.001 >= min_delta = 0.0. New best score: 0.907\n",
      "Epoch 9, global step 4099: iou_epoch reached 0.90687 (best 0.90687), saving model to \"/home/jovyan/ucb_mids_w207_final_project/notebooks/inception_v4/1r4fxc54/checkpoints/epoch=9-step=4099.ckpt\" as top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10, global step 4509: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11, global step 4919: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12, global step 5329: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13, global step 5739: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14, global step 6149: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15, global step 6559: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16, global step 6969: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17, global step 7379: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18, global step 7789: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19, global step 8199: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20, global step 8609: iou_epoch was not in top 1\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Monitored metric iou_epoch did not improve in the last 12 records. Best score: 0.907. Signaling Trainer to stop.\n",
      "Epoch 21, global step 9019: iou_epoch was not in top 1\n"
     ]
    }
   ],
   "source": [
    "# Fit the model\n",
    "trainer.fit(model = cloud_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model\n",
    "submission_assets_dir = submission_dir / \"assets\"\n",
    "submission_assets_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "model_weight_path = submission_assets_dir / \"cloud_model.pt\"\n",
    "torch.save(cloud_model.state_dict(), model_weight_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
